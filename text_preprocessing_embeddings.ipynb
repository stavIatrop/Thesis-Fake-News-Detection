{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_preprocessing_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "151eUmrikh4GxGDcJnP9eow-93EOYR22P",
      "authorship_tag": "ABX9TyOXjfIuBzdtC485JTvA1Zph",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stavIatrop/Fake-News-Detection/blob/master/text_preprocessing_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7SFzLWfKVpP",
        "colab_type": "text"
      },
      "source": [
        "Import and split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1IRgtynKeJa",
        "colab_type": "code",
        "outputId": "b8558853-b543-489c-d441-0f514de13cc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "data = pd.read_csv(\"drive/My Drive/datasets/politifact.csv\", \",\")\n",
        "data_labels = data['label'].values\n",
        "data = data['text'].values\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
        "\n",
        "for train_index, test_index in sss.split(data, data_labels):\n",
        "    X_train, X_test = data[train_index], data[test_index]\n",
        "    Y_train, Y_test = data_labels[train_index], data_labels[test_index]\n",
        "\n",
        "print(\"Train shape : \",X_train.shape)\n",
        "print(\"Test shape : \",X_test.shape)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape :  (559,)\n",
            "Test shape :  (140,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFHIPwlLTZ0z",
        "colab_type": "text"
      },
      "source": [
        "Remove non-ascii characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoGB1DM2Kqyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_non_ascii(X):\n",
        "  for i in range(len(X)):\n",
        "    words = X[i].split()\n",
        "    filtered_list = []\n",
        "    for word in words:\n",
        "        pattern = re.compile('[^\\u0000-\\u007F]+', re.UNICODE)  #Remove all non-alphanumeric characters\n",
        "        \n",
        "        word = pattern.sub('', word)\n",
        "        filtered_list.append(word)\n",
        "        result = ' '.join(filtered_list)\n",
        "        \n",
        "    X[i] = result\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k10lmzdQJMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = remove_non_ascii(X_train)\n",
        "X_test = remove_non_ascii(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA1FfNxcT7Zl",
        "colab_type": "text"
      },
      "source": [
        "Build the training vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf4ZtC9UQmtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab(sentences):     #sentences --> list of lists of tokens\n",
        "  vocab = dict()\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      if word in vocab.keys():\n",
        "        vocab[word] += 1\n",
        "      else:\n",
        "        vocab[word] = 1\n",
        "  return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xrEUAD1WQPl",
        "colab_type": "code",
        "outputId": "fb935da1-b891-4456-b831-1ab47a6eaaed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentences = [row.split() for row in X_train]\n",
        "vocab = build_vocab(sentences)\n",
        "print({k: vocab[k] for k in list(vocab)[:10]})"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'George': 177, 'W.': 41, 'Bush': 284, 'has': 2956, 'lobbed': 3, 'thinly-veiled': 1, 'critiques': 2, 'of': 20543, 'President': 1135, 'Donald': 255}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7u85SVuWuid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def load_glove_index():\n",
        "    EMBEDDING_FILE = \"/content/drive/My Drive/GloVe/glove.6B.50d.txt\"\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:50]\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
        "    return embeddings_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un8SkaNkY4IW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_index = load_glove_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk4TWKtoJFvi",
        "colab_type": "text"
      },
      "source": [
        "Check the percentage that GloVe vocab covers traing vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrDMLmBqY_8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "def check_cover(vocab, glove_index):\n",
        "\n",
        "  not_in_embeddings = dict()\n",
        "  in_embeddings = dict()\n",
        "  text_len_in = 0\n",
        "  text_len_out = 0\n",
        "  for word in vocab.keys():\n",
        "    if word in glove_index.keys():\n",
        "      in_embeddings[word] = vocab[word]\n",
        "      text_len_in += vocab[word]\n",
        "    else:\n",
        "      not_in_embeddings[word] = vocab[word]\n",
        "      text_len_out += vocab[word]\n",
        "  \n",
        "  print(\"Training vocabulary is covered at %.2f %%\" % ((len(in_embeddings)/len(vocab)) * 100 ))\n",
        "  print(\"Training text is covered at %.2f %%\" % ((text_len_in/(text_len_in + text_len_out)) * 100) )\n",
        "  \n",
        "  not_in_emb_sorted = sorted(not_in_embeddings.items(), key=operator.itemgetter(1))[::-1]\n",
        "  \n",
        "  return not_in_emb_sorted\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T-Sl4FOZ19_",
        "colab_type": "code",
        "outputId": "2856478a-9ffe-4de2-d5b9-b046c75d5831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "not_in_embeddings = check_cover(vocab=vocab, glove_index=glove_index)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training vocabulary is covered at 30.65 %\n",
            "Training text is covered at 72.05 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg_n1dE5Rait",
        "colab_type": "text"
      },
      "source": [
        "GloVe embeddings cover only ~30% of the vocabulary, which means that ~28% of our dataset is not utilized. The next step is to check the vocabulary words that are not included in the embeddings to see if we can improve things."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B5qhomDPfsF",
        "colab_type": "code",
        "outputId": "e1cd3b4e-7877-43dd-ef4f-9aa8b87492f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "not_in_embeddings[:20]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 10424),\n",
              " ('And', 4814),\n",
              " ('The', 3134),\n",
              " ('But', 2096),\n",
              " ('We', 1938),\n",
              " (\"it's\", 1210),\n",
              " ('So', 1208),\n",
              " (\"don't\", 1152),\n",
              " ('President', 1135),\n",
              " ('You', 1104),\n",
              " ('American', 1067),\n",
              " ('United', 1010),\n",
              " ('Senator', 1006),\n",
              " (\"that's\", 1006),\n",
              " ('THE', 999),\n",
              " ('It', 971),\n",
              " ('Well,', 963),\n",
              " ('know,', 921),\n",
              " (\"we're\", 898),\n",
              " ('This', 895)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGB9SYNcXuXb",
        "colab_type": "text"
      },
      "source": [
        "It seems that many words that start with capital letter are ommited from the embeddings, but are their lower case forms ommited?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgS5xjCtZDSk",
        "colab_type": "code",
        "outputId": "5593ce9d-a7d1-4d0f-e981-08c73d68fbf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'i' in glove_index"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzigmSbxSDzW",
        "colab_type": "code",
        "outputId": "d2f0b79d-141d-4dc9-b2ae-b4b90f89e87b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'and' in glove_index"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2p2JoMrSc1A",
        "colab_type": "code",
        "outputId": "f6b0cb06-b4b3-4e95-a4c2-d3e587b5484d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'the' in glove_index"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RTGTSCImg-c",
        "colab_type": "code",
        "outputId": "cdd34cbb-0faa-4003-bdf2-c4a7a3008064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'president' in glove_index"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWTKE3VAZrFy",
        "colab_type": "text"
      },
      "source": [
        "As it was suspected, the preprocessing of the embeddings may include the process of lower casing the data. So, let's transform the data into lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVmrSTKMaXrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def toLowerCase(X):\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    filtered_list = []\n",
        "    for word in X[i].split():\n",
        "      word = word.lower()\n",
        "      filtered_list.append(word)\n",
        "      result = ' '.join(filtered_list)\n",
        "\n",
        "    X[i] = result\n",
        "  \n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JH4-HLOg3v9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = toLowerCase(X_train)\n",
        "sentences = [row.split() for row in X_train]\n",
        "vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWrOfOZnhU_7",
        "colab_type": "text"
      },
      "source": [
        "Now that the text is lower case, check the coverage of embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tonA6Xokhb-8",
        "colab_type": "code",
        "outputId": "a1f40a9c-5324-40bd-8600-55e7b9a5c1ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "not_in_embeddings = check_cover(vocab=vocab, glove_index=glove_index)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training vocabulary is covered at 44.12 %\n",
            "Training text is covered at 84.67 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GK4xZXnhuJg",
        "colab_type": "text"
      },
      "source": [
        "The coverage was increased from 30% to 44%. Let's check now for words not included in embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woIuoR8BiMaS",
        "colab_type": "code",
        "outputId": "d9f01cae-eb10-4d7d-f4ce-27749ee18022",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "not_in_embeddings[:20]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"it's\", 1838),\n",
              " (\"that's\", 1635),\n",
              " (\"don't\", 1188),\n",
              " (\"we're\", 1178),\n",
              " ('well,', 1117),\n",
              " ('know,', 921),\n",
              " (\"i'm\", 848),\n",
              " ('it.', 827),\n",
              " ('now,', 806),\n",
              " (\"we've\", 748),\n",
              " ('(applause.)', 586),\n",
              " ('obama:', 577),\n",
              " ('that.', 562),\n",
              " (\"they're\", 552),\n",
              " (\"you're\", 532),\n",
              " ('that,', 493),\n",
              " ('tapper:', 487),\n",
              " (\"i've\", 485),\n",
              " (\"there's\", 480),\n",
              " ('said,', 470)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR5FYR4-ZFeC",
        "colab_type": "code",
        "outputId": "efccbecd-684f-47d2-e8ff-93cd6196d6f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'obama' in glove_index"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QJIy3gmZKOp",
        "colab_type": "code",
        "outputId": "64169321-98d5-4fa3-c86d-b24880e0f903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'know' in glove_index"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7v934ioZNf-",
        "colab_type": "code",
        "outputId": "045c0122-4725-45f7-a062-3687f004b1e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'thats' in glove_index"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXHR4fBLZQPT",
        "colab_type": "code",
        "outputId": "ae07642c-21e9-4103-ee8c-9ad465a95bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'dont' in glove_index"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64aaylhxmwCR",
        "colab_type": "code",
        "outputId": "068557af-c67f-4847-ce8d-f43475bb509a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'theres' in glove_index"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqTyEqpwmykH",
        "colab_type": "code",
        "outputId": "321e949d-2a57-4f16-dbd6-2eaf7263cb1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'\\'' in glove_index"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTf6ZJWNomSv",
        "colab_type": "code",
        "outputId": "fedf12cd-e2c1-4504-ef79-25ac5b2e2fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "':' in glove_index"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PfEmDn0pSZF",
        "colab_type": "code",
        "outputId": "bb095bba-4969-4e10-9267-e4b295da0b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "',' in glove_index"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTs3O1glpanS",
        "colab_type": "code",
        "outputId": "e36193d5-b88a-423a-9d00-57ca568855f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'(' in glove_index"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1rrEISHps45",
        "colab_type": "code",
        "outputId": "640a7b25-0527-4d27-a683-6e56b5c51439",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "').' in glove_index"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhmU5P0bpup3",
        "colab_type": "text"
      },
      "source": [
        "It seems that punctuation symbol are not totally eliminated from the preprocessing of the embeddings. So, I will remove punctuation if it is in the middle of a word token or separate it if it is at the start/end of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQB-0jrX-CpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "def handle_punctuation(X):\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    filtered_list = []\n",
        "    for word in X[i].split():\n",
        "      \n",
        "      cleaned = 0\n",
        "      \n",
        "      while(not cleaned):\n",
        "        punc_word = \"\"\n",
        "\n",
        "        if (word[0] in string.punctuation):\n",
        "          punc_word = word[0]\n",
        "          if (len(word) == 1):\n",
        "            cleaned = 1\n",
        "          else:\n",
        "            word = word[1:]\n",
        "          filtered_list.append(punc_word)\n",
        "          result = ' '.join(filtered_list)\n",
        "        elif (word[len(word) - 1] in string.punctuation):\n",
        "          punc_word = word[len(word) - 1]\n",
        "          word = word[:len(word) - 1]\n",
        "          filtered_list.append(punc_word)\n",
        "          result = ' '.join(filtered_list)\n",
        "        else:\n",
        "          word = word.translate(str.maketrans('', '', string.punctuation))\n",
        "          cleaned = 1\n",
        "          filtered_list.append(word)\n",
        "          result = ' '.join(filtered_list)\n",
        "    \n",
        "    X[i] = result\n",
        "  \n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOSrYtDy-fAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = handle_punctuation(X_train)\n",
        "sentences = [row.split() for row in X_train]\n",
        "vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRfiw1OQBP1m",
        "colab_type": "text"
      },
      "source": [
        "Check coverage now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgUGdw6zP_Yi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "408a6836-864f-4ff1-9e10-e449787191e0"
      },
      "source": [
        "not_in_embeddings = check_cover(vocab=vocab, glove_index=glove_index)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training vocabulary is covered at 81.71 %\n",
            "Training text is covered at 99.08 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt4aQNALQEqU",
        "colab_type": "text"
      },
      "source": [
        "Almost all of the text is covered! Try removing completely the punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpDzl2XBV3L0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctuation(X):\n",
        "  \n",
        "  for i in range(len(X)):\n",
        "    filtered_list = []\n",
        "    for word in X[i].split():\n",
        "      \n",
        "      word = word.translate(str.maketrans('', '', string.punctuation))\n",
        "      \n",
        "      filtered_list.append(word)\n",
        "      result = ' '.join(filtered_list)\n",
        "    \n",
        "    X[i] = result\n",
        "  \n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ag3ArniWHd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train_no_punc = remove_punctuation(X_train)\n",
        "# sentences = [row.split() for row in X_train]\n",
        "# vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsKMsuhOY_Y8",
        "colab_type": "text"
      },
      "source": [
        "Check coverage without punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rletLcSeZCXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# not_in_embeddings = check_cover(vocab=vocab, glove_index=glove_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6zytOkEZGlO",
        "colab_type": "text"
      },
      "source": [
        "Slightly lower, so we will keep the punctuation. And let's check for further improvement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B85gZtrQDKT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "3f28daef-2a0c-4370-b185-9ba6f9fc6f4b"
      },
      "source": [
        "not_in_embeddings[:20]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('youve', 351),\n",
              " ('theyve', 210),\n",
              " ('shouldnt', 102),\n",
              " ('theyll', 78),\n",
              " ('werent', 74),\n",
              " ('odonnell', 62),\n",
              " ('250000', 49),\n",
              " ('bbld', 47),\n",
              " ('whove', 39),\n",
              " ('africanamerican', 33),\n",
              " ('202n', 30),\n",
              " ('africanamericans', 29),\n",
              " ('shortterm', 29),\n",
              " ('theyd', 29),\n",
              " ('hadnt', 28),\n",
              " ('200000', 28),\n",
              " ('costsharing', 26),\n",
              " ('karibjanian', 25),\n",
              " ('doddfrank', 25),\n",
              " ('twothirds', 22)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYsVPiJyUpnR",
        "colab_type": "text"
      },
      "source": [
        "Many preprocessed embeddings have replaced large numbers with #, so let's clean the numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tDSFQusRWF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_numbers(X):\n",
        "  for i in range(len(X)):\n",
        "    x = X[i]\n",
        "    if bool(re.search(r'\\d', x)):\n",
        "      x = re.sub('[0-9]{4,}', ' ### ', x)\n",
        "      x = re.sub('[0-9]{3}', ' ## ', x)\n",
        "      x = re.sub('[0-9]{2}', ' # ', x)\n",
        "    X[i] = x\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRchH0dORw93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train = clean_numbers(X_train)\n",
        "# sentences = [row.split() for row in X_train]\n",
        "# vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNWVuiXGdRbr",
        "colab_type": "text"
      },
      "source": [
        "Check coverage with '#' instead of numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYMrSd7OcfKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# not_in_embeddings = check_cover(vocab=vocab, glove_index=glove_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu50MKJDdcMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# not_in_embeddings[:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYOTIYWFdpgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}