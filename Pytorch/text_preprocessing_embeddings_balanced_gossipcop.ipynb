{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_preprocessing_embeddings_balanced_gossipcop.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1Bnh4rVaxafH-uCzl8NvnhSJs3yfGmPfL",
      "authorship_tag": "ABX9TyPiG2Xhmb833C6KgMTuPMhL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stavIatrop/Fake-News-Detection/blob/master/text_preprocessing_embeddings_balanced_gossipcop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9Iopq1qHrwC",
        "colab_type": "text"
      },
      "source": [
        "Import and split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOWgxl-FJLs0",
        "colab_type": "code",
        "outputId": "1b33ad46-6497-4026-cd9c-90693763b68b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/My Drive/datasets/gossipcop_withPunctRevBalanced.csv\", \",\")\n",
        "data_labels = data['label'].values\n",
        "data = data['text'].values\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
        "\n",
        "for train_index, test_index in sss.split(data, data_labels):\n",
        "    X_train, X_test = data[train_index], data[test_index]\n",
        "    Y_train, Y_test = data_labels[train_index], data_labels[test_index]\n",
        "\n",
        "print(\"Train shape : \",X_train.shape)\n",
        "print(\"Test shape : \",X_test.shape)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape :  (6601,)\n",
            "Test shape :  (1651,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh308sJEICbE",
        "colab_type": "text"
      },
      "source": [
        "Remove non-ascii characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EUCpJx3IBIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_non_ascii(X):\n",
        "  for i in range(len(X)):\n",
        "    words = X[i].split()\n",
        "    filtered_list = []\n",
        "    for word in words:\n",
        "        pattern = re.compile('[^\\u0000-\\u007F]+', re.UNICODE)  #Remove all non-alphanumeric characters\n",
        "        \n",
        "        word = pattern.sub(\" \", word)\n",
        "        filtered_list.append(word)\n",
        "        result = ' '.join(filtered_list)\n",
        "        \n",
        "    X[i] = result\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N56R8JO_IISw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = remove_non_ascii(X_train)\n",
        "X_test = remove_non_ascii(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgXOJjuOIND9",
        "colab_type": "text"
      },
      "source": [
        "Build the training vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ALecwpDIOFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab(sentences):     #sentences --> list of lists of tokens\n",
        "  vocab = dict()\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      if word in vocab.keys():\n",
        "        vocab[word] += 1\n",
        "      else:\n",
        "        vocab[word] = 1\n",
        "  return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDMGiSo9IS90",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6bc74f5a-25af-4df2-9b30-c41cad63efc4"
      },
      "source": [
        "sentences = [row.split() for row in X_train]\n",
        "vocab = build_vocab(sentences)\n",
        "print({k: vocab[k] for k in list(vocab)[:10]})"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Trending': 210, 'Update:After': 1, 'we': 8927, 'received': 1064, 'EXCLUSIVE': 59, 'info': 44, 'that': 44580, 'the': 175742, 'two': 5587, 'of': 82930}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7u85SVuWuid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def load_glove_index():\n",
        "    EMBEDDING_FILE = \"/content/drive/My Drive/GloVe/glove.6B.50d.txt\"\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:50]\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
        "    return embeddings_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un8SkaNkY4IW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_index = load_glove_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwDNqHu0JOcJ",
        "colab_type": "text"
      },
      "source": [
        "Check the percentage that GloVe vocab covers training vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHhpbumlJSt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "def check_cover(vocab, glove_index):\n",
        "\n",
        "  not_in_embeddings = dict()\n",
        "  in_embeddings = dict()\n",
        "  text_len_in = 0\n",
        "  text_len_out = 0\n",
        "  for word in vocab.keys():\n",
        "    if word in glove_index.keys():\n",
        "      in_embeddings[word] = vocab[word]\n",
        "      text_len_in += vocab[word]\n",
        "    else:\n",
        "      not_in_embeddings[word] = vocab[word]\n",
        "      text_len_out += vocab[word]\n",
        "  \n",
        "  print(\"Training vocabulary is covered at %.2f %%\" % ((len(in_embeddings)/len(vocab)) * 100 ))\n",
        "  print(\"Training text is covered at %.2f %%\" % ((text_len_in/(text_len_in + text_len_out)) * 100) )\n",
        "  \n",
        "  not_in_emb_sorted = sorted(not_in_embeddings.items(), key=operator.itemgetter(1))[::-1]\n",
        "  \n",
        "  return not_in_emb_sorted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAMl2lc4JTM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d71d9d20-3a23-41fa-a12d-24c108c905c0"
      },
      "source": [
        "not_in_embeddings = check_cover(vocab=vocab, glove_index=glove_index)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training vocabulary is covered at 13.73 %\n",
            "Training text is covered at 68.27 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtuq6lZzJbhX",
        "colab_type": "text"
      },
      "source": [
        "GloVe embeddings cover only ~14% of the vocabulary, which means that ~32% of our dataset is not utilized. The next step is to check the vocabulary words that are not included in the embeddings to see if we can improve things."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BQRre_oJVOH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "1f561a5c-35e4-4fe1-89be-35bd2b455187"
      },
      "source": [
        "not_in_embeddings[:20]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 31531),\n",
              " ('The', 28208),\n",
              " ('In', 8002),\n",
              " ('She', 7484),\n",
              " ('It', 6101),\n",
              " ('He', 5474),\n",
              " ('And', 4745),\n",
              " ('A', 4643),\n",
              " ('But', 4415),\n",
              " ('This', 4197),\n",
              " ('New', 3777),\n",
              " ('We', 3760),\n",
              " ('They', 2962),\n",
              " ('You', 2905),\n",
              " ('Prince', 2544),\n",
              " ('Kardashian', 2413),\n",
              " ('Kim', 2402),\n",
              " ('\"I', 2386),\n",
              " ('On', 2317),\n",
              " ('Brad', 2270)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h9JLeHJJ0dG",
        "colab_type": "text"
      },
      "source": [
        "It seems that many words that start with capital letter are ommited from the embeddings, but are their lower case forms ommited?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y01pyQR_Jp9K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "169d7160-3a81-47cb-b8ba-2126117a783f"
      },
      "source": [
        "'i' in glove_index"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwZu-LPxKAX3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d484264-88c2-481f-b6dc-90e3e54fd9f0"
      },
      "source": [
        "'and' in glove_index"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p553GWozKDE8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b480d7c-e881-4d64-f781-1703851944f6"
      },
      "source": [
        "'prince' in glove_index"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_wWkg-YKI3M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b999cf9d-20b4-4500-e88d-784e8be4b217"
      },
      "source": [
        "'kardashian' in glove_index"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCy0TT_dKQK7",
        "colab_type": "text"
      },
      "source": [
        "As it was suspected, the preprocessing of the embeddings may include the process of lower casing the data. So, let's transform the data into lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoGQj3ddKNeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def toLowerCase(X):\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    filtered_list = []\n",
        "    for word in X[i].split():\n",
        "      word = word.lower()\n",
        "      filtered_list.append(word)\n",
        "      result = ' '.join(filtered_list)\n",
        "\n",
        "    X[i] = result\n",
        "  \n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uvIt1r5KWJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = toLowerCase(X_train)\n",
        "sentences = [row.split() for row in X_train]\n",
        "vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3dXYU7wKbxV",
        "colab_type": "text"
      },
      "source": [
        "Now that the text is lower case, check the coverage of embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy5cWekLKYY4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fa4df63b-71cb-4bb2-f85e-2f618bcc04a2"
      },
      "source": [
        "not_in_embeddings = check_cover(vocab=vocab, glove_index=glove_index)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training vocabulary is covered at 21.91 %\n",
            "Training text is covered at 83.42 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTxS7PRLKpAp",
        "colab_type": "text"
      },
      "source": [
        "The coverage was increased from 14% to ~22%. Let's check now for words not included in embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL2PSQ-fKyrG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "c71efe25-cea4-4f48-fea4-18c1e26b5c31"
      },
      "source": [
        "not_in_embeddings[:20]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"it's\", 3317),\n",
              " ('\"i', 2390),\n",
              " ('it.', 1968),\n",
              " ('it?s', 1923),\n",
              " (\"i'm\", 1843),\n",
              " (\"don't\", 1791),\n",
              " ('however,', 1551),\n",
              " ('like,', 1491),\n",
              " (\"she's\", 1438),\n",
              " ('said,', 1352),\n",
              " ('\"the', 1319),\n",
              " ('her.', 1228),\n",
              " (\"didn't\", 1225),\n",
              " ('time,', 1210),\n",
              " (\"that's\", 1206),\n",
              " ('it,', 1189),\n",
              " ('year,', 1143),\n",
              " ('?i', 1121),\n",
              " ('time.', 1062),\n",
              " (\"he's\", 1053)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC7uL3D4K0nc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd8fa666-396b-449e-f74f-8be0502a6e26"
      },
      "source": [
        "'it' in glove_index"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NopEtK1fLC7E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04bc4b68-388f-4fac-ace7-7680596f10f1"
      },
      "source": [
        "'thats' in glove_index"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW4gkxMZLCv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12b8fefa-befd-438e-cb5f-fff5095f5d4c"
      },
      "source": [
        "'dont' in glove_index"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdD_9BojLCii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ea896f5-f921-4a3d-f6e2-da47288607fe"
      },
      "source": [
        "',' in glove_index"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHvdNnkLK-QM",
        "colab_type": "text"
      },
      "source": [
        "It seems that punctuation symbol are not totally eliminated from the preprocessing of the embeddings. So, I will remove punctuation if it is in the middle of a word token or separate it if it is at the start/end of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3H3UcR4LVsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "def handle_punctuation(X):\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    filtered_list = []\n",
        "    \n",
        "    for word in X[i].split():\n",
        "      \n",
        "      cleaned = 0\n",
        "      \n",
        "      while(not cleaned):\n",
        "        punc_word = \"\"\n",
        "\n",
        "        if (word[0] in string.punctuation):\n",
        "          punc_word = word[0]\n",
        "          if (len(word) == 1):\n",
        "            cleaned = 1\n",
        "          else:\n",
        "            word = word[1:]\n",
        "          filtered_list.append(punc_word)\n",
        "          result = ' '.join(filtered_list)\n",
        "        elif (word[len(word) - 1] in string.punctuation):\n",
        "          punc_word = word[len(word) - 1]\n",
        "          word = word[:len(word) - 1]\n",
        "          filtered_list.append(punc_word)\n",
        "          result = ' '.join(filtered_list)\n",
        "        else:\n",
        "          #word = word.translate(str.maketrans(' ', ' ', string.punctuation))\n",
        "          t = str.maketrans(dict.fromkeys(string.punctuation, \" \"))\n",
        "          word = word.translate(t)\n",
        "          cleaned = 1\n",
        "          filtered_list.append(word)\n",
        "          result = ' '.join(filtered_list)\n",
        "    \n",
        "    X[i] = result\n",
        "  \n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB8M5L4ZLZE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = handle_punctuation(X_train)\n",
        "sentences = [row.split() for row in X_train]\n",
        "vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfCTQu1aLmt6",
        "colab_type": "text"
      },
      "source": [
        "Check coverage now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lV9GoOELjyW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "230d21ff-b7f6-470e-bf17-172adf1a4cb4"
      },
      "source": [
        "not_in_embeddings = check_cover(vocab=vocab, glove_index=glove_index)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training vocabulary is covered at 54.32 %\n",
            "Training text is covered at 98.41 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STiLlwGhQ3D9",
        "colab_type": "text"
      },
      "source": [
        "Almost all of the text is covered! Try removing completely the punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "087RVayNLp4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctuation(X):\n",
        "  \n",
        "  for i in range(len(X)):\n",
        "    filtered_list = []\n",
        "    for word in X[i].split():\n",
        "      \n",
        "      t = str.maketrans(dict.fromkeys(string.punctuation, \" \"))\n",
        "      word = word.translate(t)\n",
        "      filtered_list.append(word)\n",
        "      result = ' '.join(filtered_list)\n",
        "    \n",
        "    X[i] = result\n",
        "  \n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa1S3x6dRDPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train = remove_punctuation(X_train)\n",
        "# sentences = [row.split() for row in X_train]\n",
        "# vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJb6CqpkRIkD",
        "colab_type": "text"
      },
      "source": [
        "Check coverage without punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF-7xsjtRKjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1b669a71-2ae2-4335-e831-827891e61f26"
      },
      "source": [
        "#not_in_embeddings = check_cover(vocab=vocab, glove_index=glove_index)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training vocabulary is covered at 54.31 %\n",
            "Training text is covered at 98.17 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0WgPc8pRQwX",
        "colab_type": "text"
      },
      "source": [
        "Slightly lower (54.31%, 98.17% ), so we will keep the punctuation. And let's check for further improvement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VIPYbnGRMiY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "47ddd650-fabb-4afc-c277-1bd8a6722ca0"
      },
      "source": [
        "not_in_embeddings[:20]"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('aposs', 638),\n",
              " ('khlo', 395),\n",
              " ('disponible', 327),\n",
              " ('disick', 321),\n",
              " ('stormi', 305),\n",
              " ('hollywoodlife', 269),\n",
              " ('wireimage', 263),\n",
              " ('edici', 261),\n",
              " ('verlo', 260),\n",
              " ('gustar', 260),\n",
              " ('personalizado', 260),\n",
              " ('hollywoodlifers', 244),\n",
              " ('shookus', 220),\n",
              " ('hadn', 206),\n",
              " ('apost', 197),\n",
              " ('viewcomments', 177),\n",
              " ('kimkardashian', 166),\n",
              " ('selfie', 165),\n",
              " ('conservatee', 159),\n",
              " ('updated5', 154)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1kC_aQ7XTVK",
        "colab_type": "text"
      },
      "source": [
        "Many preprocessed embeddings have replaced large numbers with #, so let's clean the numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eaR_kuQRViS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_numbers(X):\n",
        "  for i in range(len(X)):\n",
        "    x = X[i]\n",
        "    if bool(re.search(r'\\d', x)):\n",
        "      x = re.sub('[0-9]{4,}', ' ### ', x)\n",
        "      x = re.sub('[0-9]{3}', ' ## ', x)\n",
        "      x = re.sub('[0-9]{2}', ' # ', x)\n",
        "    X[i] = x\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuxWAtYHW9nZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = clean_numbers(X_train)\n",
        "sentences = [row.split() for row in X_train]\n",
        "vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaST2SqMXbOe",
        "colab_type": "text"
      },
      "source": [
        "Check coverage with '#' instead of numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNF_9EBpXXwP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7c731ad2-4919-4506-ab0f-ea0162341ee1"
      },
      "source": [
        "not_in_embeddings = check_cover(vocab=vocab, glove_index=glove_index)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training vocabulary is covered at 54.53 %\n",
            "Training text is covered at 98.47 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQf1hALaXja0",
        "colab_type": "text"
      },
      "source": [
        "There is an improvement of ~0.2%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec7MTD8kXdQp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "8d9c96cb-f320-47ec-8017-11fe36ddac27"
      },
      "source": [
        "not_in_embeddings[:20]"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('aposs', 638),\n",
              " ('khlo', 395),\n",
              " ('disponible', 327),\n",
              " ('disick', 321),\n",
              " ('stormi', 305),\n",
              " ('hollywoodlife', 269),\n",
              " ('wireimage', 263),\n",
              " ('edici', 261),\n",
              " ('verlo', 260),\n",
              " ('gustar', 260),\n",
              " ('personalizado', 260),\n",
              " ('hollywoodlifers', 244),\n",
              " ('shookus', 220),\n",
              " ('hadn', 206),\n",
              " ('apost', 197),\n",
              " ('viewcomments', 177),\n",
              " ('kimkardashian', 166),\n",
              " ('selfie', 165),\n",
              " ('conservatee', 159),\n",
              " ('updated5', 154)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn0p0-EVXsvB",
        "colab_type": "text"
      },
      "source": [
        "Until here, the improvement increased from 14% to 54% and there is no obvious further improvement. The training data is ready."
      ]
    }
  ]
}